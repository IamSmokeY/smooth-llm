MODELS = {
    'llama2': {
        'model_path': 'meta-llama/Llama-2-7b-chat-hf',
        'tokenizer_path': 'meta-llama/Llama-2-7b-chat-hf',
        'conversation_template': 'llama-2'
    },
    'llama3.1': {
        'model_path': 'meta-llama/Meta-Llama-3.1-8B-Instruct',
        'tokenizer_path': 'meta-llama/Meta-Llama-3.1-8B-Instruct',
        'conversation_template': 'llama-3'
    },
    'vicuna': {
        'model_path': 'lmsys/vicuna-13b-v1.3',
        'tokenizer_path': 'lmsys/vicuna-13b-v1.3',
        'conversation_template': 'vicuna'
    }
}
